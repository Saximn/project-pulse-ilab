{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "310a2350",
   "metadata": {},
   "source": [
    "# monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43a821d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5f0a1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langfuse client is authenticated and ready!\n"
     ]
    }
   ],
   "source": [
    "from langfuse import get_client\n",
    " \n",
    "langfuse = get_client()\n",
    " \n",
    "# Verify connection\n",
    "if langfuse.auth_check():\n",
    "    print(\"Langfuse client is authenticated and ready!\")\n",
    "else:\n",
    "    print(\"Authentication failed. Please check your credentials and host.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee1599ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openinference.instrumentation.smolagents import SmolagentsInstrumentor\n",
    " \n",
    "SmolagentsInstrumentor().instrument()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51082de8",
   "metadata": {},
   "source": [
    "# start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5e1abc",
   "metadata": {},
   "source": [
    "## visualization adjust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3770e0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Tuple\n",
    "from datasets import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)  # This will be helpful when visualizing retriever outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a2631b",
   "metadata": {},
   "source": [
    "## PDF Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146ca676",
   "metadata": {},
   "source": [
    "### Normal OCR with PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95b0bf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"../assets/guidebooks/guidebook.pdf\"\n",
    "loader = PyPDFLoader(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c1c0c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a96be3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a17bd74252184d63a811b61a39b293f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/370 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "\n",
    "RAW_KNOWLEDGE_BASE = [\n",
    "    LangchainDocument(page_content=doc.page_content, metadata={\"source\": doc.metadata[\"page_label\"]}) for doc in tqdm(docs)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8220ac3",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20388cc9",
   "metadata": {},
   "source": [
    "See ```https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb ``` for other chunking methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3930fcef",
   "metadata": {},
   "source": [
    "### RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f9c15ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# We use a hierarchical list of separators specifically tailored for splitting Markdown documents\n",
    "# This list is taken from LangChain's MarkdownTextSplitter class\n",
    "MARKDOWN_SEPARATORS = [\n",
    "    \"\\n#{1,6} \",\n",
    "    \"```\\n\",\n",
    "    \"\\n\\\\*\\\\*\\\\*+\\n\",\n",
    "    \"\\n---+\\n\",\n",
    "    \"\\n___+\\n\",\n",
    "    \"\\n\\n\",\n",
    "    \"\\n\",\n",
    "    \" \",\n",
    "    \"\",\n",
    "]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # The maximum number of characters in a chunk: we selected this value arbitrarily\n",
    "    chunk_overlap=100,  # The number of characters to overlap between chunks\n",
    "    add_start_index=True,  # If `True`, includes chunk's start index in metadata\n",
    "    strip_whitespace=True,  # If `True`, strips whitespace from the start and end of every document\n",
    "    separators=MARKDOWN_SEPARATORS,\n",
    ")\n",
    "\n",
    "docs_processed = []\n",
    "for doc in RAW_KNOWLEDGE_BASE:\n",
    "    docs_processed += text_splitter.split_documents([doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920c137a",
   "metadata": {},
   "source": [
    "SentenceTransformer model could be changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df7af071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# EMBEDDING_MODEL_NAME = \"BAAI/bge-en-icl\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_NAME)\n",
    "# lengths = [len(tokenizer.encode(doc.page_content)) for doc in tqdm(docs_processed)]\n",
    "\n",
    "# # Plot the distribution of document lengths, counted as the number of tokens\n",
    "# fig = pd.Series(lengths).hist()\n",
    "# plt.title(\"Distribution of document lengths in the knowledge base (in count of tokens)\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4e69a6",
   "metadata": {},
   "source": [
    "Document chunk is forced to be 512, probably there's a better way to do this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0e0294",
   "metadata": {},
   "source": [
    "## Vector Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fa626d",
   "metadata": {},
   "source": [
    "### FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf73cbc1",
   "metadata": {},
   "source": [
    "use modal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f5cabf",
   "metadata": {},
   "source": [
    "## Use OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43baa365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d5a548c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# index = faiss.IndexFlatL2(len(embeddings.embed_query(\"hello world\")))\n",
    "\n",
    "# vector_store = FAISS(\n",
    "#     embedding_function=embeddings,\n",
    "#     index=index,\n",
    "#     docstore=InMemoryDocstore(),\n",
    "#     index_to_docstore_id={},\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11630df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_store = FAISS.from_documents(\n",
    "#     documents=RAW_KNOWLEDGE_BASE,\n",
    "#     embedding=embeddings,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b918b3f",
   "metadata": {},
   "source": [
    "Save to local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1eb428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_store.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69e59f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.load_local(\n",
    "    \"faiss_index\", embeddings, allow_dangerous_deserialization=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54f40467",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_type=\"mmr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74e77b5",
   "metadata": {},
   "source": [
    "Retriever tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fd76c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import Tool\n",
    "\n",
    "class PMBOKRetrieverTool(Tool):\n",
    "    name = \"PMBOKRetriever\"\n",
    "    description = \"Uses semantic search to retrieve the parts of Project Management Body of Knowledge (PMBOK) documentation that could be most relevant to answer your query.\"\n",
    "    inputs = {\n",
    "        \"query\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The query to perform. This should be semantically close to your target documents. Use the affirmative form rather than a question.\",\n",
    "        }\n",
    "    }\n",
    "    output_type = \"string\"\n",
    "\n",
    "    def __init__(self, vector_store, k=4, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # Initialize the retriever with our processed documents\n",
    "        self.retriever = vector_store.as_retriever(\n",
    "            search_type=\"mmr\",\n",
    "            k=k\n",
    "        )\n",
    "\n",
    "    def forward(self, query: str) -> str:\n",
    "        \"\"\"Execute the retrieval based on the provided query.\"\"\"\n",
    "        assert isinstance(query, str), \"Your search query must be a string\"\n",
    "\n",
    "        # Retrieve relevant documents\n",
    "        docs = self.retriever.invoke(query)\n",
    "\n",
    "        # Format the retrieved documents for readability\n",
    "        return \"\\nRetrieved documents:\\n\" + \"\".join(\n",
    "            [\n",
    "                f\"\\n\\n===== Document {str(i)} =====\\n\" + doc.page_content\n",
    "                for i, doc in enumerate(docs)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "# Initialize our retriever tool with the processed documents\n",
    "retriever_tool = PMBOKRetrieverTool(vector_store, k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7c522943",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import InferenceClientModel, CodeAgent\n",
    "\n",
    "# Initialize the agent with our retriever tool\n",
    "agent = CodeAgent(\n",
    "    tools=[retriever_tool],  # List of tools available to the agent\n",
    "    model=InferenceClientModel(provider=\"nebius\"),  # Default model \"Qwen/Qwen2.5-Coder-32B-Instruct\"\n",
    "    max_steps=4,  # Limit the number of reasoning steps\n",
    "    verbosity_level=2,  # Show detailed agent reasoning\n",
    ")\n",
    "\n",
    "# To use a specific model, you can specify it like this:\n",
    "# model=InferenceClientModel(model_id=\"meta-llama/Llama-3.3-70B-Instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f834471f",
   "metadata": {},
   "source": [
    "## Test 1 - 26/06/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e968cb",
   "metadata": {},
   "source": [
    "commisionning does not exist in pmbok\n",
    "\n",
    "nanya pendapat skrg\n",
    "\n",
    "add memory now\n",
    "\n",
    "it seems that the model does not know the sequence of a project.\n",
    "\n",
    "jangan kaku, kayak ngomong ke orang. end productnya mau jadi kayak manusia\n",
    "\n",
    "chat memory: ask the user for follow up tasks \"would u like a tea with that?\"\n",
    "\n",
    "needs another agent: project maker"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
